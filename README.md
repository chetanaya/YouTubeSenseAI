# YouTubeSenseAI: Comment & Transcript Analysis Dashboard

YouTubeSenseAI is a Streamlit dashboard designed to analyze YouTube videos. It allows users to search for videos, fetch comments and transcripts, perform sentiment analysis, generate summaries, and interact with video content using AI.

## Features

The application is divided into three main sections accessible via the sidebar navigation:

### 1. YouTube Comment Analysis

*   **Video Search:** Search YouTube based on keywords.
*   **Video Selection:** Select multiple videos from search results for analysis.
*   **Comment Fetching:** Retrieve comments (sorted by popularity) for selected videos.
*   **Comment Display:** View comments grouped by video.
*   **Word Clouds:** Generate word clouds from comments for each video to visualize frequent terms.
*   **AI Sentiment Analysis:**
    *   Analyze the sentiment (positive, negative, neutral) and score of individual comments using an LLM (default: `gpt-4o-mini`).
    *   Visualize overall sentiment distribution (pie chart) and score distribution (histogram).
    *   Identify and display key positive and negative phrases using AI.
    *   Visualize key phrases with a heatmap.
*   **Engagement Metrics:**
    *   Display the top 10 most upvoted comments for each video.
    *   (Partially Implemented) Visualize comment activity over time.
*   **AI Summary & Insights:**
    *   Generate an overall summary of comment sentiment, key topics, and potential recommendations using an LLM.
    *   Display extracted key topics and common positive/negative phrases.
*   **Data Download:** Download fetched comments and sentiment analysis results as CSV files.

### 2. Transcript Analysis (Standard YouTube API)

*   **URL Input:** Analyze a specific video by pasting its YouTube URL.
*   **Transcript Fetching:**
    *   Retrieve transcripts using the `youtube-transcript-api`.
    *   Supports multiple languages with automatic fallback if the preferred language isn't found.
*   **Transcript Display:**
    *   Show the full transcript text with **clickable timestamps** to navigate the video.
*   **AI Summarization:**
    *   Generate a comprehensive summary of the transcript using an LLM (supports OpenAI models and local Ollama models like `deepseek-r1`).
    *   Option to provide a custom summarization prompt.
*   **AI Q&A Generation:** Automatically generate potential questions and answers based on the transcript content.
*   **Translation:** Translate the transcript or the summary into various languages using an LLM.
*   **Interactive Q&A (RAG):**
    *   Ask questions about the video content in a chat interface.
    *   Uses a Retrieval-Augmented Generation (RAG) system (built with LangChain and LangGraph) to answer questions based on the transcript content.
    *   Supports multiple LLMs (OpenAI/Ollama).
    *   Maintains chat history per video session.
*   **Download:** Download the raw transcript text and the generated summary.

### 3. Transcript Analysis (Whisper Edition)

*   **URL Input:** Analyze a specific video by pasting its YouTube URL.
*   **Transcript Fetching (Whisper):**
    *   Downloads the audio from the YouTube video using `yt-dlp`.
    *   Uses **OpenAI Whisper** to transcribe the audio, providing a potentially more accurate transcript than standard YouTube captions.
    *   Requires `ffmpeg` to be installed on the system for audio processing.
*   **Transcript Display:**
    *   Show the full raw transcript text generated by Whisper.
*   **AI Summarization:**
    *   Generate a comprehensive summary of the Whisper transcript using an LLM.
    *   Option to provide a custom summarization prompt.
*   **AI Q&A Generation:** Automatically generate potential questions and answers based on the Whisper transcript content.
*   **Translation:** Translate the transcript or the summary into various languages using an LLM.
*   **Interactive Q&A (RAG):**
    *   Ask questions about the video content in a chat interface.
    *   Uses a Retrieval-Augmented Generation (RAG) system (built with LangChain and LangGraph) to answer questions based *only* on the **Whisper transcript** content.
    *   Maintains chat history per video session.
*   **Download:** Download the raw transcript text and the generated summary.

## Setup

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/chetanaya/YouTubeSenseAI.git
    cd YouTubeSenseAI
    ```
2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv venv
    # On Windows
    venv\\Scripts\\activate
    # On macOS/Linux
    source venv/bin/activate
    ```
3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    **Note:** The **Whisper Transcript Analysis** feature requires `ffmpeg` to be installed and accessible in your system's PATH for audio processing.
    *   **macOS (using Homebrew):** `brew install ffmpeg`
    *   **Debian/Ubuntu:** `sudo apt update && sudo apt install ffmpeg`
    *   **Windows:** Download from [ffmpeg.org](https://ffmpeg.org/download.html) and add the `bin` directory to your system's PATH.
4.  **Set up API Keys:**
    *   Copy the `.env.example` file (if it exists) or create a new file named `.env` in the project root directory.
    *   Add your OpenAI API key to the `.env` file:
        ```dotenv
        OPENAI_API_KEY=your_openai_api_key_here
        ```
    *   Alternatively, you can enter the API key directly in the application's sidebar when prompted.

## Usage

Run the Streamlit application from your terminal:

```bash
streamlit run app.py
```

Navigate through the sidebar to switch between the "YouTube Comment Analysis", "Transcript Analysis", and "Transcript Analysis (Whisper)" dashboards. Follow the on-screen instructions to search for videos, fetch data, and perform analyses.

## Configuration

Basic settings can be adjusted in the `config.yaml` file:

*   `max_videos`: Default maximum number of videos to fetch in search results.
*   `max_comments`: Default maximum number of comments to fetch per video.
*   `wordcloud_max_words`: Maximum words in the generated word clouds.
*   `title_max_chars`: Maximum characters to display for video titles in the search results.
*   `stopwords`: A list of common words to exclude from word clouds.

## Dependencies

The application uses several key libraries:

*   `streamlit`, `streamlit-scroll-to-top`: For building the web application interface and UI enhancements.
*   `pandas`, `numpy`: For data manipulation.
*   `plotly`, `matplotlib`, `wordcloud`: For data visualization.
*   `youtube-search`: For searching YouTube videos.
*   `youtube-comment-downloader`: For fetching YouTube comments.
*   `youtube-transcript-api`: For fetching standard YouTube transcripts.
*   `langchain`, `langchain-community`, `langchain-openai`, `langchain-ollama`, `openai`, `ollama`: For Large Language Model integration (summarization, sentiment analysis, Q&A, Whisper transcription, local LLM support).
*   `langgraph`: For building the RAG agent state machine.
*   `faiss-cpu`: For vector storage in the RAG system.
*   `yt-dlp`, `pydub`, `librosa`: For downloading and processing YouTube audio (used by Whisper). (`pytube` is an indirect dependency).
*   `ffmpeg` (System dependency): Required by Whisper for audio processing.
*   `pyyaml`: For loading configuration.
*   `python-dotenv`: For managing environment variables (API keys).

## Future Enhancements

1.  Enhancing video-based RAG beyond just transcripts (e.g., analyzing visual elements).
2.  Extracting key information directly from video content (visuals, audio cues).
3.  Expanding sentiment analysis to include transcript and audio emotion recognition, complementing the existing comment analysis.
4.  Implementing speaker diarization for transcripts to differentiate between speakers.
