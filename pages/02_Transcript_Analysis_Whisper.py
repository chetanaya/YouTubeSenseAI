import streamlit as st
import re
import os
import time
import tempfile
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from dotenv import load_dotenv
from typing import List
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langchain_community.document_loaders.blob_loaders.youtube_audio import (
    YoutubeAudioLoader,
)
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParser
import shutil  # Import shutil to check for ffmpeg
from modules.nav import Navbar

# Load environment variables
load_dotenv()

# Set page configuration
st.set_page_config(
    page_title="YouTube Transcript Analysis Dashboard (Using OpenAI Whisper)",
    page_icon="üé¨",
    layout="wide",
    initial_sidebar_state="expanded",
)

Navbar()


def extract_video_id(url):
    """Extract YouTube video ID from various URL formats"""
    patterns = [
        r"(?:https?:\/\/)?(?:www\.)?youtube\.com\/watch\?v=([^&\s]+)",
        r"(?:https?:\/\/)?(?:www\.)?youtu\.be\/([^\?\s]+)",
        r"(?:https?:\/\/)?(?:www\.)?youtube\.com\/embed\/([^\?\s]+)",
        r"(?:https?:\/\/)?(?:www\.)?youtube\.com\/v\/([^\?\s]+)",
        r"(?:https?:\/\/)?(?:www\.)?youtube\.com\/shorts\/([^\?\s]+)",
    ]

    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)

    return None


def fetch_transcript_whisper(video_url: str) -> str:
    """Fetch transcript from a YouTube video using OpenAI Whisper"""
    if not os.environ.get("OPENAI_API_KEY"):
        st.error("OpenAI API Key not found. Please set it in the sidebar.")
        return None

    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            loader = GenericLoader(
                YoutubeAudioLoader([video_url], temp_dir),
                OpenAIWhisperParser(
                    api_key=os.environ.get("OPENAI_API_KEY")
                ),  # Pass API key explicitly if needed
            )
            docs = loader.load()
            if docs:
                # Combine content from all documents (though usually there's one for audio)
                transcript_text = "\n".join([doc.page_content for doc in docs])
                return transcript_text.strip()
            else:
                st.error("Whisper transcription returned no documents.")
                return None
    except Exception as e:
        st.error(f"Error fetching or transcribing audio: {str(e)}")
        # Add more specific error handling if needed (e.g., for pytube errors)
        if "OPENAI_API_KEY" in str(e):
            st.error(
                "Please ensure your OpenAI API Key is correct and has Whisper access."
            )
        elif "video unavailable" in str(e).lower():
            st.error("The video might be unavailable or private.")
        return None


def summarize_transcript(transcript_text, llm_model="gpt-4o-mini", custom_prompt=None):
    """Generate a summary and potential Q&A pairs of the transcript using LLM"""
    if not transcript_text or len(transcript_text) < 50:
        return {"summary": "Transcript is too short to summarize.", "qa_pairs": ""}

    llm = ChatOpenAI(temperature=0, model=llm_model)
    output_parser = StrOutputParser()

    max_length = 15000  # Consider Whisper transcript length
    if len(transcript_text) > max_length:
        transcript_text = transcript_text[:max_length] + "..."

    # --- Summary Generation ---
    summary_prompt_template = (
        custom_prompt
        if custom_prompt
        else """
    Analyze the following YouTube video transcript (generated by Whisper) and generate a comprehensive yet concise summary.

    Transcript: {transcript}

    **Comprehensive Summary Generation Process:**

    1.  **Core Topic & Purpose:** Identify the main subject and the video's primary goal (e.g., inform, teach, persuade, review, entertain).
    2.  **Context & Audience:** Determine the setting (interview, lecture, tutorial, vlog) and intended audience, if discernible.
    3.  **Key Figures:** Note any important speakers, interviewees, or mentioned individuals.
    4.  **Structure & Flow:** Understand how the video is organized (e.g., introduction, key sections, conclusion; chronological order; problem/solution).
    5.  **Main Points & Arguments:** Extract the key messages, central arguments, steps demonstrated, or significant events discussed.
    6.  **Supporting Details & Keywords:** Include crucial data, evidence, examples, or specific terminology/keywords central to the topic.
    7.  **Actions, Recommendations & Solutions:** Capture any calls to action, solutions proposed, tips, or specific advice given.
    8.  **Broader Themes & Implications:** Recognize underlying themes, the video's relevance or timeliness, and any future predictions or outlooks mentioned.

    **Formatting and Style Guidelines:**

    *   **Objectivity:** Report neutrally what the transcript says. Attribute claims or opinions to the video's speakers (e.g., "The speaker argues...").
    *   **Conciseness:** Focus on the most valuable information for someone who hasn't seen the video. Omit fluff, excessive detail, and repetitive points.
    *   **Clarity:** Use clear headings/subheadings (Markdown). Employ bullet points for lists (key takeaways, steps). Use brief paragraphs for context.
    *   **Emphasis:** **Bold** critical terms, conclusions, or key takeaways.

    **Generated Summary:**
    """
    )
    summary_prompt = ChatPromptTemplate.from_template(summary_prompt_template)
    summary_chain = summary_prompt | llm | output_parser

    # --- Q&A Generation ---
    qa_prompt_template = """
    Based *only* on the provided YouTube video transcript (generated by Whisper), generate 3-5 insightful questions that a viewer might have after watching. For each question, provide a concise answer derived *directly* from the transcript content. Do not add information not present in the transcript.

    Transcript: {transcript}

    Format the Q&A pairs clearly:
    **Q1:** [Question 1]?
    **A1:** [Answer 1 derived from transcript].

    **Q2:** [Question 2]?
    **A2:** [Answer 2 derived from transcript].
    ...

    **Generated Q&A Pairs:**
    """
    qa_prompt = ChatPromptTemplate.from_template(qa_prompt_template)
    qa_chain = qa_prompt | llm | output_parser

    summary_result = "Error generating summary."
    qa_result = "Error generating Q&A."

    try:
        # Invoke summary chain
        summary_result = summary_chain.invoke({"transcript": transcript_text})
    except Exception as e:
        st.error(f"Error generating summary: {str(e)}")
        summary_result = "Unable to generate summary due to an error."

    try:
        # Invoke Q&A chain
        qa_result = qa_chain.invoke({"transcript": transcript_text})
    except Exception as e:
        st.error(f"Error generating Q&A: {str(e)}")
        qa_result = "Unable to generate Q&A pairs due to an error."

    return {"summary": summary_result.strip(), "qa_pairs": qa_result.strip()}


def translate_text(text, target_language, llm_model="gpt-4o-mini"):
    """Translate text to the target language using LLM"""
    if not text:
        return "No text to translate."

    llm = ChatOpenAI(temperature=0, model=llm_model)

    max_length = 15000
    if len(text) > max_length:
        text = text[:max_length] + "..."

    prompt = ChatPromptTemplate.from_template(
        """Translate the following text to {language}. Maintain the formatting and structure as much as possible:

        {text}

        Translation:
        """
    )

    chain = prompt | llm

    try:
        response = chain.invoke({"language": target_language, "text": text})
        return response.content
    except Exception as e:
        st.error(f"Error translating text: {str(e)}")
        return "Unable to translate text due to an error."


def create_rag_system(transcript_text: str, llm_model: str = "gpt-4o-mini"):
    """Create a RAG system with chat history for transcript Q&A"""
    if not transcript_text:
        st.error("Cannot create RAG system: Transcript text is empty.")
        return None

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    chunks = text_splitter.split_text(transcript_text)

    if not chunks:
        st.error("Cannot create RAG system: Failed to split transcript into chunks.")
        return None

    try:
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_texts(chunks, embeddings)
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    except Exception as e:
        st.error(f"Failed to create vector store for RAG: {e}")
        return None

    llm = ChatOpenAI(temperature=0, model=llm_model)

    def retrieve(state):
        query = (
            state["messages"][-1].content
            if isinstance(state["messages"][-1], HumanMessage)
            else ""
        )
        if not query:
            return {"context": ""}  # Avoid error if query is empty
        try:
            docs = retriever.invoke(query)
            context = "\n\n".join([doc.page_content for doc in docs])
            return {"context": context}
        except Exception as e:
            st.warning(f"Error during retrieval: {e}")
            return {"context": ""}

    def generate_answer(state):
        context = state.get("context", "")
        human_message = next(
            (m for m in reversed(state["messages"]) if isinstance(m, HumanMessage)),
            None,
        )
        if not human_message:
            return {
                "messages": state["messages"]
                + [
                    AIMessage(
                        content="I couldn't understand your question. Please try again."
                    )
                ]
            }

        messages = [
            AIMessage(
                content="I am an AI assistant that helps answer questions about the YouTube video transcript generated by Whisper."
            ),
            HumanMessage(
                content=f"""First, analyze the provided context from the video transcript and identify 2-3 key potential questions and their answers that could be derived from this specific context.

            Context:
            {context}

            Potential Q&A based on context:
            1. Q: ... A: ...
            2. Q: ... A: ...
            3. Q: ... A: ...

            Now, answer the user's question based *only* on the provided context. If the context doesn't contain the answer, state that clearly. Do not make up information.

            User's Question: {human_message.content}

            Final Answer:"""
            ),
        ]
        try:
            answer = llm.invoke(messages).content
            return {"messages": state["messages"] + [AIMessage(content=answer)]}
        except Exception as e:
            st.error(f"Error generating answer: {e}")
            return {
                "messages": state["messages"]
                + [
                    AIMessage(
                        content="Sorry, I encountered an error while generating the answer."
                    )
                ]
            }

    from typing import TypedDict

    class GraphState(TypedDict):
        messages: List
        context: str

    workflow = StateGraph(state_schema=GraphState)

    workflow.add_node("retrieve", retrieve)
    workflow.add_node("generate_answer", generate_answer)

    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "generate_answer")
    workflow.add_edge("generate_answer", END)

    rag_chain = workflow.compile(checkpointer=MemorySaver())

    def invoke_rag_chain(messages, thread_id: str):
        config = {"configurable": {"thread_id": thread_id}}
        initial_state = {"messages": messages, "context": ""}
        try:
            result = rag_chain.invoke(initial_state, config=config)
            return (
                result["messages"][-1]
                if result and result.get("messages")
                else AIMessage(content="No response generated.")
            )
        except Exception as e:
            st.error(f"Error invoking RAG chain: {e}")
            return AIMessage(
                content="Sorry, I encountered an error processing your request."
            )

    return invoke_rag_chain


def app():
    st.title("üìù YouTube Transcript Analysis (Whisper Edition)")

    st.markdown(
        """
        <style>
        .big-font {
            font-size:20px !important;
            font-weight: bold;
        }
        .container {
            background-color: #f0f2f6;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
        }
        .summary-container {
            background-color: #e6f3ff;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 20px;
            border-left: 5px solid #2196F3;
        }
        .transcript-container {
            max-height: 400px;
            overflow-y: auto;
            padding: 15px;
            background-color: #f9f9f9;
            border-radius: 5px;
            border: 1px solid #ddd;
            font-family: monospace; /* Keep monospace for readability */
            white-space: pre-wrap; /* Ensure wrapping */
        }
        .stTabs [data-baseweb="tab-list"] {
            gap: 24px;
        }
        .stTabs [data-baseweb="tab"] {
            height: 50px;
            white-space: pre-wrap;
            background-color: white;
            border-radius: 4px 4px 0px 0px;
            gap: 1px;
            padding: 10px 16px;
            font-weight: 600;
        }
        .stTabs [aria-selected="true"] {
            background-color: #e6f3ff !important;
            border-bottom: 2px solid #2196F3 !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    with st.sidebar:
        st.header("API Settings")
        openai_api_key = os.getenv("OPENAI_API_KEY", "")
        if not openai_api_key:
            openai_api_key = st.text_input("OpenAI API Key", type="password")
            if openai_api_key:
                os.environ["OPENAI_API_KEY"] = openai_api_key
            else:
                st.warning("OpenAI API Key is required for transcription and analysis.")
        elif "OPENAI_API_KEY" not in os.environ:
            os.environ["OPENAI_API_KEY"] = (
                openai_api_key  # Ensure it's set if provided via env
            )

        # Add ffmpeg check and info message
        st.header("System Dependencies")
        ffmpeg_path = shutil.which("ffmpeg")
        if not ffmpeg_path:
            st.warning(
                """
                **FFmpeg not found.**

                FFmpeg is required for processing audio files downloaded from YouTube.
                Please install it to ensure proper functionality.

                **Installation:**
                - **macOS (using Homebrew):** `brew install ffmpeg`
                - **Debian/Ubuntu:** `sudo apt update && sudo apt install ffmpeg`
                - **Windows:** Download from [ffmpeg.org](https://ffmpeg.org/download.html) and add to your system's PATH.

                After installation, please restart the Streamlit app.
                """
            )
        else:
            st.success(f"‚úÖ FFmpeg found at: `{ffmpeg_path}`")

        st.header("LLM Model Settings")
        llm_model = st.selectbox(
            "LLM Model for Analysis",
            ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4", "gpt-4o"],
            index=0,
        )
        # Removed transcript language selection

    col1, col2 = st.columns([3, 1])
    with col1:
        st.markdown(
            '<p class="big-font">Enter YouTube Video URL</p>', unsafe_allow_html=True
        )
        video_url = st.text_input(
            "Paste the YouTube video URL here",
            placeholder="https://www.youtube.com/watch?v=...",
        )
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        st.markdown("<br>", unsafe_allow_html=True)

    if video_url:
        video_id = extract_video_id(video_url)

        if not video_id:
            st.error("Invalid YouTube URL. Please enter a valid YouTube video URL.")
        else:
            st.markdown(
                f'<iframe width="100%" height="500" src="https://www.youtube.com/embed/{video_id}" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>',
                unsafe_allow_html=True,
            )

            with st.expander("Advanced Summarization Options", expanded=False):
                custom_prompt = st.text_area(
                    "Custom Summarization Prompt (leave empty to use default)",
                    height=100,
                    help="Customize how the transcript is summarized. Use {transcript} as a placeholder for the transcript text.",
                )
                if not custom_prompt:
                    custom_prompt = None

            fetch_btn = st.button(
                "üé§ Fetch Audio & Analyze Transcript (Whisper)",
                type="primary",
                use_container_width=True,
            )

            # Initialize session state keys
            if "transcript_text" not in st.session_state:
                st.session_state.transcript_text = ""
            if "summary" not in st.session_state:
                st.session_state.summary = ""
            if "qa_pairs" not in st.session_state:
                st.session_state.qa_pairs = ""
            if "current_video_id" not in st.session_state:
                st.session_state.current_video_id = None
            if "chat_histories" not in st.session_state:
                st.session_state.chat_histories = {}
            if "rag_chains" not in st.session_state:
                st.session_state.rag_chains = {}

            if fetch_btn:
                if not os.environ.get("OPENAI_API_KEY"):
                    st.error("Please enter your OpenAI API Key in the sidebar first.")
                    st.stop()

                if st.session_state.current_video_id != video_id:
                    # Reset state for new video
                    st.session_state.current_video_id = video_id
                    st.session_state.transcript_text = ""
                    st.session_state.summary = ""
                    st.session_state.qa_pairs = ""
                    st.session_state.chat_histories[video_id] = []
                    st.session_state.rag_chains[video_id] = None

                with st.status(
                    "Processing video with Whisper...", expanded=True
                ) as status:
                    st.write(
                        "Downloading audio and transcribing (this may take a while)..."
                    )
                    transcript_text = fetch_transcript_whisper(video_url)

                    if transcript_text:
                        st.session_state.transcript_text = transcript_text
                        st.write("‚úÖ Transcription complete!")
                        time.sleep(0.5)

                        st.write("Generating summary and Q&A insights...")
                        analysis_result = summarize_transcript(
                            st.session_state.transcript_text, llm_model, custom_prompt
                        )
                        st.session_state.summary = analysis_result.get(
                            "summary", "Summary generation failed."
                        )
                        st.session_state.qa_pairs = analysis_result.get(
                            "qa_pairs", "Q&A generation failed."
                        )
                        st.write("‚úÖ Summary and Q&A generated.")
                        time.sleep(0.5)

                        st.write("Setting up question answering system...")
                        rag_chain_instance = create_rag_system(
                            st.session_state.transcript_text, llm_model
                        )
                        if rag_chain_instance:
                            st.session_state.rag_chains[video_id] = rag_chain_instance
                            st.write("‚úÖ RAG system ready.")
                        else:
                            st.warning("Could not initialize RAG system.")
                            st.session_state.rag_chains[video_id] = None

                        if video_id not in st.session_state.chat_histories:
                            st.session_state.chat_histories[video_id] = []

                        status.update(
                            label="‚úÖ Analysis complete!",
                            state="complete",
                            expanded=False,
                        )
                    else:
                        status.update(label="‚ùå Transcription failed", state="error")
                        st.error("Could not get transcript using Whisper.")
                        # Clear potentially stale data
                        st.session_state.transcript_text = ""
                        st.session_state.summary = ""
                        st.session_state.qa_pairs = ""
                        if video_id in st.session_state.rag_chains:
                            st.session_state.rag_chains[video_id] = None
                        st.session_state.current_video_id = (
                            None  # Reset current video ID on failure
                        )

            current_video_id = st.session_state.get("current_video_id")
            # Check if the current video ID matches and transcript text exists
            if (
                current_video_id
                and current_video_id == video_id
                and st.session_state.transcript_text
            ):
                tabs = st.tabs(
                    [
                        "üìä Summary",
                        "üí° Q&A Insights",
                        "üìú Transcript (Raw)",  # Changed tab name
                        "üåê Translations",
                    ]
                )

                with tabs[0]:  # Summary Tab
                    if st.session_state.summary:
                        with st.container():
                            summary_container = st.expander(
                                "Full Summary", expanded=True
                            )
                            with summary_container:
                                st.markdown(st.session_state.summary)

                        st.download_button(
                            label="üì• Download Summary",
                            data=st.session_state.summary,
                            file_name=f"summary_whisper_{video_id}.md",
                            mime="text/markdown",
                        )
                    else:
                        st.info("Summary will appear here once generated.")

                with tabs[1]:  # Q&A Insights Tab
                    st.subheader("üí° Q&A Insights")
                    if st.session_state.qa_pairs:
                        st.info(
                            "Here are some potential questions and answers based on the video transcript:"
                        )
                        st.markdown(st.session_state.qa_pairs)
                    elif st.session_state.summary and not st.session_state.qa_pairs:
                        st.warning(
                            "Q&A insights could not be generated for this video."
                        )
                    else:
                        st.info(
                            "Q&A insights will appear here once the transcript is analyzed."
                        )

                with tabs[2]:  # Raw Transcript Tab
                    st.subheader("üìú Raw Transcript (from Whisper)")
                    with st.container():
                        transcript_container = st.expander(
                            "Full Raw Transcript", expanded=True
                        )
                        with transcript_container:
                            st.text(st.session_state.transcript_text)

                    st.download_button(
                        label="üì• Download Raw Transcript",
                        data=st.session_state.transcript_text,
                        file_name=f"transcript_whisper_{video_id}.txt",
                        mime="text/plain",
                    )

                with tabs[3]:  # Translations Tab
                    col1, col2 = st.columns(2)

                    with col1:
                        st.subheader("Translate Transcript")
                        target_language_transcript = st.selectbox(
                            "Select language",
                            options=[
                                "English",
                                "Spanish",
                                "French",
                                "German",
                                "Italian",
                                "Portuguese",
                                "Russian",
                                "Japanese",
                                "Korean",
                                "Chinese",
                                "Arabic",
                                "Hindi",
                            ],
                            key="translate_transcript_lang",
                        )

                        translate_transcript_btn = st.button(
                            f"Translate Transcript to {target_language_transcript}",
                            use_container_width=True,
                            key="transcript_translate_btn",
                        )

                        if (
                            translate_transcript_btn
                            and st.session_state.transcript_text
                        ):
                            with st.spinner(
                                f"Translating to {target_language_transcript}..."
                            ):
                                translated_transcript = translate_text(
                                    st.session_state.transcript_text,
                                    target_language_transcript,
                                    llm_model,
                                )
                                # Display translated transcript in a styled container
                                st.markdown(
                                    '<div class="transcript-container">',
                                    unsafe_allow_html=True,
                                )
                                st.markdown(translated_transcript)
                                st.markdown("</div>", unsafe_allow_html=True)

                    with col2:
                        st.subheader("Translate Summary")
                        target_language_summary = st.selectbox(
                            "Select language",
                            options=[
                                "English",
                                "Spanish",
                                "French",
                                "German",
                                "Italian",
                                "Portuguese",
                                "Russian",
                                "Japanese",
                                "Korean",
                                "Chinese",
                                "Arabic",
                                "Hindi",
                            ],
                            key="translate_summary_lang",
                        )

                        translate_summary_btn = st.button(
                            f"Translate Summary to {target_language_summary}",
                            use_container_width=True,
                            key="summary_translate_btn",
                        )

                        if translate_summary_btn and st.session_state.summary:
                            with st.spinner(
                                f"Translating to {target_language_summary}..."
                            ):
                                translated_summary = translate_text(
                                    st.session_state.summary,
                                    target_language_summary,
                                    llm_model,
                                )
                                with (
                                    st.container()
                                ):  # Use a simple container for summary
                                    st.markdown(translated_summary)

                # --- Chat Section ---
                st.markdown("---")
                st.subheader("üí¨ Ask Questions About This Video (Whisper Transcript)")

                st.info(
                    "Ask questions about the video content, and I'll use the Whisper transcript to answer them. "
                    "Accuracy depends on the Whisper transcription quality."
                )

                current_chat_history = st.session_state.chat_histories.get(video_id, [])

                # Display chat history
                for message in current_chat_history:
                    with st.chat_message(message["role"]):
                        st.markdown(message["content"])

                def response_streamer(response_text):
                    for word in response_text.split():
                        yield word + " "
                        time.sleep(0.01)

                # Chat input
                if prompt := st.chat_input("Ask a question based on the transcript..."):
                    current_rag_chain = st.session_state.rag_chains.get(video_id)

                    if not st.session_state.transcript_text:
                        st.error(
                            "Please fetch and analyze the transcript first before asking questions."
                        )
                        st.stop()
                    if not current_rag_chain:
                        st.error(
                            "The question answering system (RAG) is not available. Please try fetching/analyzing again."
                        )
                        st.stop()

                    # Add user message to history and display it
                    current_chat_history.append({"role": "user", "content": prompt})
                    with st.chat_message("user"):
                        st.markdown(prompt)

                    # Get assistant response
                    with st.chat_message("assistant"):
                        with st.spinner("Thinking..."):
                            # Prepare messages for the RAG chain
                            messages_for_chain = []
                            for msg in current_chat_history:
                                if msg["role"] == "user":
                                    messages_for_chain.append(
                                        HumanMessage(content=msg["content"])
                                    )
                                elif msg["role"] == "assistant":
                                    messages_for_chain.append(
                                        AIMessage(content=msg["content"])
                                    )

                            thread_id = f"vid_whisper_{video_id}"  # Unique thread ID
                            result = current_rag_chain(
                                messages_for_chain, thread_id=thread_id
                            )
                            response = (
                                result.content
                                if hasattr(result, "content")
                                else "Sorry, I couldn't generate a response."  # Fallback
                            )

                            st.write_stream(response_streamer(response))

                    # Add assistant response to history
                    current_chat_history.append(
                        {"role": "assistant", "content": response}
                    )
                    st.session_state.chat_histories[video_id] = (
                        current_chat_history  # Save updated history
                    )


if __name__ == "__main__":
    app()
